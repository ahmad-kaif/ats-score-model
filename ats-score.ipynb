{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8566750-dcc6-4ac0-85c7-65805178b6e6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f6334fe-e79d-4d23-a5f1-57caaa9ed6f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in ./.venv/lib/python3.13/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.13/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.13/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.13/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./.venv/lib/python3.13/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.13/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.13/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./.venv/lib/python3.13/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./.venv/lib/python3.13/site-packages (from spacy) (2.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.venv/lib/python3.13/site-packages (from spacy) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.13/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.13/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.13/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in ./.venv/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in ./.venv/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./.venv/lib/python3.13/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4a4a520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1028)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/Ahmad/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/share/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/corpus/util.py:84\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/Ahmad/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/share/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m             nltk.download(key)\n\u001b[32m     57\u001b[39m ensure_nltk_resources()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m STOP_WORDS = \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# ---  Make sure spaCy model is available\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/corpus/util.py:120\u001b[39m, in \u001b[36mLazyCorpusLoader.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLazyCorpusLoader object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/corpus/util.py:86\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m             root = nltk.data.find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[32m     89\u001b[39m corpus = \u001b[38;5;28mself\u001b[39m.__reader_cls(root, *\u001b[38;5;28mself\u001b[39m.__args, **\u001b[38;5;28mself\u001b[39m.__kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/corpus/util.py:81\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/Ahmad/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/share/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "End-to-end ATS-score pipeline\n",
    "–––––––––––––––––––––––––––––\n",
    "1. Cleans + lemmatises Resume & Job-Description text\n",
    "2. Creates sentence-BERT embeddings\n",
    "3. Computes weighted (70-30) ATS score:\n",
    "      70 % similarity(Resume, JD)  +\n",
    "      30 % similarity(Resume, Role)\n",
    "4. Prints and optionally saves results\n",
    "\"\"\"\n",
    "\n",
    "# ───────────────────────────── Imports\n",
    "import re\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP pre-processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "# Embeddings & maths\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download all necessary corpora\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "# ───────────────────────────── House-keeping\n",
    "\n",
    "DATA_FPATH = \"dataset.csv\"          # ← change if needed\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"     # fast, 384-d vector model\n",
    "SPACY_MODEL = \"en_core_web_sm\"      # lemmatisation model\n",
    "DEBOUNCE_DELAY = 600                # ms for UI typing – kept here for reference\n",
    "\n",
    "# ---  Make sure required NLTK artefacts exist only once\n",
    "def ensure_nltk_resources() -> None:\n",
    "    resources = {\n",
    "        \"punkt\": \"tokenizers/punkt\",\n",
    "        \"stopwords\": \"corpora/stopwords\",\n",
    "    }\n",
    "    for key, target in resources.items():\n",
    "        try:\n",
    "            nltk.data.find(target)\n",
    "        except LookupError:\n",
    "            nltk.download(key)\n",
    "\n",
    "ensure_nltk_resources()\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# ---  Make sure spaCy model is available\n",
    "try:\n",
    "    _ = spacy.load(SPACY_MODEL)\n",
    "except OSError:  # model missing\n",
    "    from spacy.cli import download\n",
    "    download(SPACY_MODEL)\n",
    "\n",
    "nlp = spacy.load(SPACY_MODEL, disable=[\"ner\", \"parser\"])  # we only need tagger+lemmatizer\n",
    "\n",
    "# ─────────────────────────────  Text helpers\n",
    "_re_non_word = re.compile(r\"\\W+\")\n",
    "_re_digit    = re.compile(r\"\\d+\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lower-case, drop punctuation & numbers, strip stop-words.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = _re_non_word.sub(\" \", text)   # punctuation → space\n",
    "    text = _re_digit.sub(\" \", text)      # numbers → space\n",
    "    tokens = [\n",
    "        w for w in word_tokenize(text)\n",
    "        if w not in STOP_WORDS and len(w) > 1            # keep tokens >1 char\n",
    "    ]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def lemmatise(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatise via spaCy (‘en_core_web_sm’).\n",
    "    \"\"\"\n",
    "    return \" \".join(tok.lemma_ for tok in nlp(text))\n",
    "\n",
    "\n",
    "def preprocess_series(ser: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Pipeline: clean → lemmatise.\n",
    "    \"\"\"\n",
    "    return ser.fillna(\"\").astype(str).map(clean_text).map(lemmatise)\n",
    "\n",
    "\n",
    "# ─────────────────────────────  Main\n",
    "def main() -> None:\n",
    "    # 1️⃣  Load data\n",
    "    if not pathlib.Path(DATA_FPATH).exists():\n",
    "        raise FileNotFoundError(f\"{DATA_FPATH} not found – check path.\")\n",
    "    df = pd.read_csv(DATA_FPATH)\n",
    "\n",
    "    required_cols = {\"Resume\", \"Job_Description\", \"Role\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        missing = required_cols - set(df.columns)\n",
    "        raise KeyError(f\"CSV missing columns: {', '.join(missing)}\")\n",
    "\n",
    "    # 2️⃣  Clean + lemmatise (vectorised via Pandas)\n",
    "    print(\"🔄  Pre-processing text …\")\n",
    "    df[\"Resume_Clean\"] = preprocess_series(df[\"Resume\"])\n",
    "    df[\"JD_Clean\"]     = preprocess_series(df[\"Job_Description\"])\n",
    "\n",
    "    # 3️⃣  Sentence-BERT embeddings\n",
    "    print(\"🔄  Encoding with Sentence-BERT …\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    resume_vecs = model.encode(df[\"Resume_Clean\"].tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
    "    jd_vecs     = model.encode(df[\"JD_Clean\"].tolist(),     convert_to_numpy=True, show_progress_bar=True)\n",
    "    role_vecs   = model.encode(df[\"Role\"].fillna(\"\").tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "    # 4️⃣  Similarities\n",
    "    sim_res_jd   = cosine_similarity(resume_vecs, jd_vecs).diagonal()\n",
    "    sim_res_role = cosine_similarity(resume_vecs, role_vecs).diagonal()\n",
    "\n",
    "    # 5️⃣  Normalise both sets 0–1, then weighted score\n",
    "    scaler = MinMaxScaler()\n",
    "    sim_res_jd   = scaler.fit_transform(sim_res_jd.reshape(-1, 1)).ravel()\n",
    "    sim_res_role = scaler.fit_transform(sim_res_role.reshape(-1, 1)).ravel()\n",
    "\n",
    "    df[\"ATS_Score\"] = (0.7 * sim_res_jd + 0.3 * sim_res_role) * 100\n",
    "\n",
    "    # 6️⃣  Output\n",
    "    cols_to_show = [\"Name\", \"Role\", \"ATS_Score\"]\n",
    "    if \"decision\" in df.columns:\n",
    "        cols_to_show.insert(2, \"decision\")\n",
    "\n",
    "    print(\"\\n🎯  Results:\")\n",
    "    print(df[cols_to_show].sort_values(\"ATS_Score\", ascending=False).to_string(index=False))\n",
    "\n",
    "    # Uncomment if you want to save\n",
    "    # df.to_csv(\"ats_scores_with_role.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cce97e66-9142-4c78-967c-fb1b00df0feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re #to remove punctuations(regex)\n",
    "import nltk # natural language tool kit\n",
    "import spacy\n",
    "from nltk.corpus import stopwords # text preprocessing\n",
    "from nltk.tokenize import word_tokenize # text preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # text -> vector\n",
    "from sklearn.metrics.pairwise import cosine_similarity # compare two vectors how closely they are related\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "205c0776-a8db-4452-aa2e-5dd7886944af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4a5fced-216b-4ac2-8226-0c8f5690c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10174 entries, 0 to 10173\n",
      "Data columns (total 8 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   ID                   10174 non-null  object\n",
      " 1   Name                 10174 non-null  object\n",
      " 2   Role                 10174 non-null  object\n",
      " 3   Transcript           10174 non-null  object\n",
      " 4   Resume               10174 non-null  object\n",
      " 5   decision             10174 non-null  object\n",
      " 6   Reason_for_decision  10174 non-null  object\n",
      " 7   Job_Description      10174 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 636.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd95d63e-d4ab-49cc-8802-d1ee3f963972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Role</th>\n",
       "      <th>Transcript</th>\n",
       "      <th>Resume</th>\n",
       "      <th>decision</th>\n",
       "      <th>Reason_for_decision</th>\n",
       "      <th>Job_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jasojo159</td>\n",
       "      <td>Jason Jones</td>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Interviewer: Good morning, Jason. It's great t...</td>\n",
       "      <td>Here's a professional resume for Jason Jones:\\...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Lacked leadership skills for a senior position.</td>\n",
       "      <td>Be part of a passionate team at the forefront ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>annma759</td>\n",
       "      <td>Ann Marshall</td>\n",
       "      <td>Game Developer</td>\n",
       "      <td>Interview Scene\\n\\nA conference room with a ta...</td>\n",
       "      <td>Here's a professional resume for Ann Marshall:...</td>\n",
       "      <td>select</td>\n",
       "      <td>Strong technical skills in AI and ML.</td>\n",
       "      <td>Help us build the next-generation products as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patrmc729</td>\n",
       "      <td>Patrick Mcclain</td>\n",
       "      <td>Human Resources Specialist</td>\n",
       "      <td>Interview Setting: A conference room in a medi...</td>\n",
       "      <td>Here's a professional resume for Patrick Mccla...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Insufficient system design expertise for senio...</td>\n",
       "      <td>We need a Human Resources Specialist to enhanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patrgr422</td>\n",
       "      <td>Patricia Gray</td>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Here's a simulated professional interview for ...</td>\n",
       "      <td>Here's a professional resume for Patricia Gray...</td>\n",
       "      <td>select</td>\n",
       "      <td>Impressive leadership and communication abilit...</td>\n",
       "      <td>Be part of a passionate team at the forefront ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amangr696</td>\n",
       "      <td>Amanda Gross</td>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Here's the simulated interview:\\n\\nInterviewer...</td>\n",
       "      <td>Here's a professional resume for Amanda Gross:...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Lacked leadership skills for a senior position.</td>\n",
       "      <td>We are looking for an experienced E-commerce S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID             Name                        Role  \\\n",
       "0  jasojo159      Jason Jones       E-commerce Specialist   \n",
       "1   annma759     Ann Marshall              Game Developer   \n",
       "2  patrmc729  Patrick Mcclain  Human Resources Specialist   \n",
       "3  patrgr422    Patricia Gray       E-commerce Specialist   \n",
       "4  amangr696     Amanda Gross       E-commerce Specialist   \n",
       "\n",
       "                                          Transcript  \\\n",
       "0  Interviewer: Good morning, Jason. It's great t...   \n",
       "1  Interview Scene\\n\\nA conference room with a ta...   \n",
       "2  Interview Setting: A conference room in a medi...   \n",
       "3  Here's a simulated professional interview for ...   \n",
       "4  Here's the simulated interview:\\n\\nInterviewer...   \n",
       "\n",
       "                                              Resume decision  \\\n",
       "0  Here's a professional resume for Jason Jones:\\...   reject   \n",
       "1  Here's a professional resume for Ann Marshall:...   select   \n",
       "2  Here's a professional resume for Patrick Mccla...   reject   \n",
       "3  Here's a professional resume for Patricia Gray...   select   \n",
       "4  Here's a professional resume for Amanda Gross:...   reject   \n",
       "\n",
       "                                 Reason_for_decision  \\\n",
       "0    Lacked leadership skills for a senior position.   \n",
       "1              Strong technical skills in AI and ML.   \n",
       "2  Insufficient system design expertise for senio...   \n",
       "3  Impressive leadership and communication abilit...   \n",
       "4    Lacked leadership skills for a senior position.   \n",
       "\n",
       "                                     Job_Description  \n",
       "0  Be part of a passionate team at the forefront ...  \n",
       "1  Help us build the next-generation products as ...  \n",
       "2  We need a Human Resources Specialist to enhanc...  \n",
       "3  Be part of a passionate team at the forefront ...  \n",
       "4  We are looking for an experienced E-commerce S...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33e43218-97cc-4725-8656-244481a4799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1028)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  # For lemmatization\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "631d2273-e4ba-46f0-93ff-941e770ff635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    words = word_tokenize(text)  # Tokenize\n",
    "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc54140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42d19493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fe9511d-2401-46d5-bd75-6cfd8337b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b04a80b-b93c-4bf1-bcba-135e83f15c33",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Ahmad/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/share/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Apply preprocessing to Resume & Job Description\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mResume_Clean\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResume\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m.apply(lemmatize_text)\n\u001b[32m      3\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mJob_Desc_Clean\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mJob_Description\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).apply(clean_text).apply(lemmatize_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mclean_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      5\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mW+\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove special characters\u001b[39;00m\n\u001b[32m      6\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md+\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove numbers\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m words = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenize\u001b[39;00m\n\u001b[32m      8\u001b[39m words = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m)]  \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(words)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Ahmad/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/share/nltk_data'\n    - '/Users/Ahmad/Desktop/MyProjects/ats-score-model/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to Resume & Job Description\n",
    "df[\"Resume_Clean\"] = df[\"Resume\"].astype(str).apply(clean_text).apply(lemmatize_text)\n",
    "df[\"Job_Desc_Clean\"] = df[\"Job_Description\"].astype(str).apply(clean_text).apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f17590-8557-4bad-9319-287f2120d43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Role</th>\n",
       "      <th>Transcript</th>\n",
       "      <th>Resume</th>\n",
       "      <th>decision</th>\n",
       "      <th>Reason_for_decision</th>\n",
       "      <th>Job_Description</th>\n",
       "      <th>Resume_Clean</th>\n",
       "      <th>Job_Desc_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jasojo159</td>\n",
       "      <td>Jason Jones</td>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Interviewer: Good morning, Jason. It's great t...</td>\n",
       "      <td>Here's a professional resume for Jason Jones:\\...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Lacked leadership skills for a senior position.</td>\n",
       "      <td>Be part of a passionate team at the forefront ...</td>\n",
       "      <td>professional resume jason jones jason jones e ...</td>\n",
       "      <td>part passionate team forefront machine learn e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>annma759</td>\n",
       "      <td>Ann Marshall</td>\n",
       "      <td>Game Developer</td>\n",
       "      <td>Interview Scene\\n\\nA conference room with a ta...</td>\n",
       "      <td>Here's a professional resume for Ann Marshall:...</td>\n",
       "      <td>select</td>\n",
       "      <td>Strong technical skills in AI and ML.</td>\n",
       "      <td>Help us build the next-generation products as ...</td>\n",
       "      <td>professional resume ann marshall ann marshall ...</td>\n",
       "      <td>help we build next generation product game dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patrmc729</td>\n",
       "      <td>Patrick Mcclain</td>\n",
       "      <td>Human Resources Specialist</td>\n",
       "      <td>Interview Setting: A conference room in a medi...</td>\n",
       "      <td>Here's a professional resume for Patrick Mccla...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Insufficient system design expertise for senio...</td>\n",
       "      <td>We need a Human Resources Specialist to enhanc...</td>\n",
       "      <td>professional resume patrick mcclain patrick mc...</td>\n",
       "      <td>need human resource specialist enhance team te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patrgr422</td>\n",
       "      <td>Patricia Gray</td>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Here's a simulated professional interview for ...</td>\n",
       "      <td>Here's a professional resume for Patricia Gray...</td>\n",
       "      <td>select</td>\n",
       "      <td>Impressive leadership and communication abilit...</td>\n",
       "      <td>Be part of a passionate team at the forefront ...</td>\n",
       "      <td>professional resume patricia gray patricia gra...</td>\n",
       "      <td>part passionate team forefront cloud computing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amangr696</td>\n",
       "      <td>Amanda Gross</td>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Here's the simulated interview:\\n\\nInterviewer...</td>\n",
       "      <td>Here's a professional resume for Amanda Gross:...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Lacked leadership skills for a senior position.</td>\n",
       "      <td>We are looking for an experienced E-commerce S...</td>\n",
       "      <td>professional resume amanda gross amanda gross ...</td>\n",
       "      <td>look experience e commerce specialist join tea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID             Name                        Role  \\\n",
       "0  jasojo159      Jason Jones       E-commerce Specialist   \n",
       "1   annma759     Ann Marshall              Game Developer   \n",
       "2  patrmc729  Patrick Mcclain  Human Resources Specialist   \n",
       "3  patrgr422    Patricia Gray       E-commerce Specialist   \n",
       "4  amangr696     Amanda Gross       E-commerce Specialist   \n",
       "\n",
       "                                          Transcript  \\\n",
       "0  Interviewer: Good morning, Jason. It's great t...   \n",
       "1  Interview Scene\\n\\nA conference room with a ta...   \n",
       "2  Interview Setting: A conference room in a medi...   \n",
       "3  Here's a simulated professional interview for ...   \n",
       "4  Here's the simulated interview:\\n\\nInterviewer...   \n",
       "\n",
       "                                              Resume decision  \\\n",
       "0  Here's a professional resume for Jason Jones:\\...   reject   \n",
       "1  Here's a professional resume for Ann Marshall:...   select   \n",
       "2  Here's a professional resume for Patrick Mccla...   reject   \n",
       "3  Here's a professional resume for Patricia Gray...   select   \n",
       "4  Here's a professional resume for Amanda Gross:...   reject   \n",
       "\n",
       "                                 Reason_for_decision  \\\n",
       "0    Lacked leadership skills for a senior position.   \n",
       "1              Strong technical skills in AI and ML.   \n",
       "2  Insufficient system design expertise for senio...   \n",
       "3  Impressive leadership and communication abilit...   \n",
       "4    Lacked leadership skills for a senior position.   \n",
       "\n",
       "                                     Job_Description  \\\n",
       "0  Be part of a passionate team at the forefront ...   \n",
       "1  Help us build the next-generation products as ...   \n",
       "2  We need a Human Resources Specialist to enhanc...   \n",
       "3  Be part of a passionate team at the forefront ...   \n",
       "4  We are looking for an experienced E-commerce S...   \n",
       "\n",
       "                                        Resume_Clean  \\\n",
       "0  professional resume jason jones jason jones e ...   \n",
       "1  professional resume ann marshall ann marshall ...   \n",
       "2  professional resume patrick mcclain patrick mc...   \n",
       "3  professional resume patricia gray patricia gra...   \n",
       "4  professional resume amanda gross amanda gross ...   \n",
       "\n",
       "                                      Job_Desc_Clean  \n",
       "0  part passionate team forefront machine learn e...  \n",
       "1  help we build next generation product game dev...  \n",
       "2  need human resource specialist enhance team te...  \n",
       "3  part passionate team forefront cloud computing...  \n",
       "4  look experience e commerce specialist join tea...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e9b38-8587-40ae-aeb0-0bea7945452a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name                        Role decision  ATS_Score\n",
      "0          Jason Jones       E-commerce Specialist   reject   9.226798\n",
      "1         Ann Marshall              Game Developer   select   5.624799\n",
      "2      Patrick Mcclain  Human Resources Specialist   reject  16.415510\n",
      "3        Patricia Gray       E-commerce Specialist   select   7.105350\n",
      "4         Amanda Gross       E-commerce Specialist   reject   3.949370\n",
      "...                ...                         ...      ...        ...\n",
      "10169     Diana Miller             Product Manager   reject  59.163743\n",
      "10170     Grace Taylor                 UI Engineer   reject  45.243484\n",
      "10171       Hank Brown                 UI Engineer   select  41.368585\n",
      "10172     Diana Wilson               Data Engineer   reject  44.630917\n",
      "10173   Charlie Miller             Product Manager   select  60.052097\n",
      "\n",
      "[10174 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import spacy\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# # Initialize TF-IDF Vectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Fit & transform Resume and Job Description\n",
    "# tfidf_resume = tfidf_vectorizer.fit_transform(df[\"Resume_Clean\"])\n",
    "# tfidf_job_desc = tfidf_vectorizer.transform(df[\"Job_Desc_Clean\"])\n",
    "\n",
    "# # Compute Cosine Similarity\n",
    "# similarity_scores = cosine_similarity(tfidf_resume, tfidf_job_desc)\n",
    "\n",
    "# # Extract diagonal values as ATS scores\n",
    "# df[\"ATS_Score\"] = similarity_scores.diagonal() * 100  # Convert to percentage\n",
    "\n",
    "# # Display results\n",
    "# print(df[[\"Name\", \"Role\", \"decision\", \"ATS_Score\"]])\n",
    "\n",
    "# # # Save results to CSV (optional)\n",
    "# # df.to_csv(\"ats_scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66818955-328f-4762-9f32-43c09eb6b9c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name                        Role decision  ATS_Score\n",
      "0          Jason Jones       E-commerce Specialist   reject  37.113407\n",
      "1         Ann Marshall              Game Developer   select  32.107269\n",
      "2      Patrick Mcclain  Human Resources Specialist   reject  48.492916\n",
      "3        Patricia Gray       E-commerce Specialist   select  37.091381\n",
      "4         Amanda Gross       E-commerce Specialist   reject  41.472687\n",
      "...                ...                         ...      ...        ...\n",
      "10169     Diana Miller             Product Manager   reject  83.335228\n",
      "10170     Grace Taylor                 UI Engineer   reject  87.547577\n",
      "10171       Hank Brown                 UI Engineer   select  81.156395\n",
      "10172     Diana Wilson               Data Engineer   reject  78.613815\n",
      "10173   Charlie Miller             Product Manager   select  83.494476\n",
      "\n",
      "[10174 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import spacy\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Load the BERT-based sentence transformer model\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# # Convert Resume & Job Descriptions to embeddings\n",
    "# resume_embeddings = model.encode(df[\"Resume_Clean\"], convert_to_numpy=True)\n",
    "# job_desc_embeddings = model.encode(df[\"Job_Desc_Clean\"], convert_to_numpy=True)\n",
    "\n",
    "# # Compute Cosine Similarity\n",
    "# similarity_scores = cosine_similarity(resume_embeddings, job_desc_embeddings)\n",
    "\n",
    "# # Extract diagonal values as ATS scores\n",
    "# df[\"ATS_Score\"] = similarity_scores.diagonal() * 100  # Convert to percentage\n",
    "\n",
    "# # Display results\n",
    "# print(df[[\"Name\", \"Role\", \"decision\", \"ATS_Score\"]])\n",
    "\n",
    "# # # Save results to CSV (optional)\n",
    "# # df.to_csv(\"ats_scores_improved.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef983f8-eba5-46b0-8c57-15e186169e07",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name                        Role decision  ATS_Score\n",
      "0          Jason Jones       E-commerce Specialist   reject  47.348263\n",
      "1         Ann Marshall              Game Developer   select  43.961040\n",
      "2      Patrick Mcclain  Human Resources Specialist   reject  57.735325\n",
      "3        Patricia Gray       E-commerce Specialist   select  46.574200\n",
      "4         Amanda Gross       E-commerce Specialist   reject  48.630505\n",
      "...                ...                         ...      ...        ...\n",
      "10169     Diana Miller             Product Manager   reject  77.520844\n",
      "10170     Grace Taylor                 UI Engineer   reject  83.561493\n",
      "10171       Hank Brown                 UI Engineer   select  77.151260\n",
      "10172     Diana Wilson               Data Engineer   reject  78.636681\n",
      "10173   Charlie Miller             Product Manager   select  82.010712\n",
      "\n",
      "[10174 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Load the BERT-based sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert Resume, Job Descriptions, and Role to embeddings\n",
    "resume_embeddings = model.encode(df[\"Resume_Clean\"], convert_to_numpy=True)\n",
    "job_desc_embeddings = model.encode(df[\"Job_Desc_Clean\"], convert_to_numpy=True)\n",
    "role_embeddings = model.encode(df[\"Role\"], convert_to_numpy=True)  # New step for Role\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "similarity_resume_jd = cosine_similarity(resume_embeddings, job_desc_embeddings).diagonal()\n",
    "similarity_resume_role = cosine_similarity(resume_embeddings, role_embeddings).diagonal()  # New similarity\n",
    "\n",
    "# Normalize both similarity scores\n",
    "scaler = MinMaxScaler()\n",
    "similarity_resume_jd = scaler.fit_transform(similarity_resume_jd.reshape(-1, 1)).flatten()\n",
    "similarity_resume_role = scaler.fit_transform(similarity_resume_role.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Compute the final ATS score (weighted sum)\n",
    "df[\"ATS_Score\"] = (0.7 * similarity_resume_jd + 0.3 * similarity_resume_role) * 100  # 70-30 weight split\n",
    "\n",
    "# Display results\n",
    "print(df[[\"Name\", \"Role\", \"decision\", \"ATS_Score\"]])\n",
    "\n",
    "# # Save results to CSV (optional)\n",
    "# df.to_csv(\"ats_scores_with_role.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351243c-3348-424a-8ae3-57d31e7bebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Name                        Role  ATS_Score\n",
      "0          Jason Jones       E-commerce Specialist  44.386967\n",
      "1         Ann Marshall              Game Developer  40.343868\n",
      "2      Patrick Mcclain  Human Resources Specialist  55.369049\n",
      "3        Patricia Gray       E-commerce Specialist  43.863071\n",
      "4         Amanda Gross       E-commerce Specialist  46.796097\n",
      "...                ...                         ...        ...\n",
      "10169     Diana Miller             Product Manager  80.982475\n",
      "10170     Grace Taylor                 UI Engineer  86.511490\n",
      "10171       Hank Brown                 UI Engineer  79.959213\n",
      "10172     Diana Wilson               Data Engineer  80.042938\n",
      "10173   Charlie Miller             Product Manager  84.032494\n",
      "\n",
      "[10174 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the BERT-based sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert Resume, Job Descriptions, and Role to embeddings\n",
    "resume_embeddings = model.encode(df[\"Resume_Clean\"], convert_to_numpy=True)\n",
    "job_desc_embeddings = model.encode(df[\"Job_Desc_Clean\"], convert_to_numpy=True)\n",
    "role_embeddings = model.encode(df[\"Role\"], convert_to_numpy=True)\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "similarity_resume_jd = cosine_similarity(resume_embeddings, job_desc_embeddings).diagonal()\n",
    "similarity_resume_role = cosine_similarity(resume_embeddings, role_embeddings).diagonal()\n",
    "\n",
    "# Normalize scores\n",
    "scaler = MinMaxScaler()\n",
    "similarity_resume_jd = scaler.fit_transform(similarity_resume_jd.reshape(-1, 1)).flatten()\n",
    "similarity_resume_role = scaler.fit_transform(similarity_resume_role.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Compute final ATS score (weighted sum)\n",
    "df[\"ATS_Score\"] = (0.8 * similarity_resume_jd + 0.2 * similarity_resume_role) * 100\n",
    "\n",
    "# 🔹 Extract Important Keywords using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10)  # Top 10 keywords\n",
    "job_desc_keywords = vectorizer.fit_transform(df[\"Job_Desc_Clean\"])\n",
    "resume_keywords = vectorizer.transform(df[\"Resume_Clean\"])\n",
    "\n",
    "# # Generate suggestions\n",
    "# def generate_suggestions(index):\n",
    "#     missing_keywords = set(vectorizer.get_feature_names_out()) - set(df[\"Resume_Clean\"][index].split())\n",
    "#     suggestions = []\n",
    "    \n",
    "#     if similarity_resume_jd[index] < 0.6:  # If job description match is low\n",
    "#         suggestions.append(\"Include more relevant experience matching the job description.\")\n",
    "    \n",
    "#     if similarity_resume_role[index] < 0.5:  # If role match is low\n",
    "#         suggestions.append(f\"Tailor your resume for the role '{df['Role'][index]}'.\")\n",
    "\n",
    "#     if missing_keywords:\n",
    "#         suggestions.append(f\"Consider adding these skills/keywords: {', '.join(missing_keywords)}.\")\n",
    "\n",
    "#     return \" \".join(suggestions) if suggestions else \"Your resume is well-matched!\"\n",
    "\n",
    "# df[\"Suggestions\"] = df.index.map(generate_suggestions)\n",
    "\n",
    "# Display results\n",
    "# print(df[[\"Name\", \"Role\", \"ATS_Score\", \"Suggestions\"]])\n",
    "print(df[[\"Name\", \"Role\", \"ATS_Score\"]])\n",
    "\n",
    "# # Save results to CSV (optional)\n",
    "# df.to_csv(\"ats_scores_with_suggestions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64898fe5-fa41-49b7-9408-679712ae7dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10174 entries, 0 to 10173\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   ID                   10174 non-null  object \n",
      " 1   Name                 10174 non-null  object \n",
      " 2   Role                 10174 non-null  object \n",
      " 3   Transcript           10174 non-null  object \n",
      " 4   Resume               10174 non-null  object \n",
      " 5   decision             10174 non-null  object \n",
      " 6   Reason_for_decision  10174 non-null  object \n",
      " 7   Job_Description      10174 non-null  object \n",
      " 8   Resume_Clean         10174 non-null  object \n",
      " 9   Job_Desc_Clean       10174 non-null  object \n",
      " 10  ATS_Score            10174 non-null  float32\n",
      "dtypes: float32(1), object(10)\n",
      "memory usage: 834.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0ba20-25f4-4bb8-b9fb-94aa63016391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"ID\", \"Name\", \"Transcript\", \"Resume\", \"decision\", \"Reason_for_decision\", \"Job_Description\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb9512c-efd4-4652-9bfc-5b11d8a50d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Role</th>\n",
       "      <th>Resume_Clean</th>\n",
       "      <th>Job_Desc_Clean</th>\n",
       "      <th>ATS_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>professional resume jason jones jason jones e ...</td>\n",
       "      <td>part passionate team forefront machine learn e...</td>\n",
       "      <td>44.386967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Game Developer</td>\n",
       "      <td>professional resume ann marshall ann marshall ...</td>\n",
       "      <td>help we build next generation product game dev...</td>\n",
       "      <td>40.343868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Human Resources Specialist</td>\n",
       "      <td>professional resume patrick mcclain patrick mc...</td>\n",
       "      <td>need human resource specialist enhance team te...</td>\n",
       "      <td>55.369049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>professional resume patricia gray patricia gra...</td>\n",
       "      <td>part passionate team forefront cloud computing...</td>\n",
       "      <td>43.863071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>professional resume amanda gross amanda gross ...</td>\n",
       "      <td>look experience e commerce specialist join tea...</td>\n",
       "      <td>46.796097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Role  \\\n",
       "0       E-commerce Specialist   \n",
       "1              Game Developer   \n",
       "2  Human Resources Specialist   \n",
       "3       E-commerce Specialist   \n",
       "4       E-commerce Specialist   \n",
       "\n",
       "                                        Resume_Clean  \\\n",
       "0  professional resume jason jones jason jones e ...   \n",
       "1  professional resume ann marshall ann marshall ...   \n",
       "2  professional resume patrick mcclain patrick mc...   \n",
       "3  professional resume patricia gray patricia gra...   \n",
       "4  professional resume amanda gross amanda gross ...   \n",
       "\n",
       "                                      Job_Desc_Clean  ATS_Score  \n",
       "0  part passionate team forefront machine learn e...  44.386967  \n",
       "1  help we build next generation product game dev...  40.343868  \n",
       "2  need human resource specialist enhance team te...  55.369049  \n",
       "3  part passionate team forefront cloud computing...  43.863071  \n",
       "4  look experience e commerce specialist join tea...  46.796097  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88238376-bc35-4d98-803b-371b07db6d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique roles: 45\n",
      "\n",
      "Top 10 most common roles:\n",
      "Role\n",
      "Data Scientist       538\n",
      "Software Engineer    480\n",
      "Product Manager      458\n",
      "Data Engineer        447\n",
      "UI Engineer          375\n",
      "Data Analyst         329\n",
      "data engineer        307\n",
      "software engineer    307\n",
      "product manager      303\n",
      "data scientist       287\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution (top 10):\n",
      "Role\n",
      "Data Scientist       5.287989\n",
      "Software Engineer    4.717908\n",
      "Product Manager      4.501671\n",
      "Data Engineer        4.393552\n",
      "UI Engineer          3.685866\n",
      "Data Analyst         3.233733\n",
      "data engineer        3.017496\n",
      "software engineer    3.017496\n",
      "product manager      2.978180\n",
      "data scientist       2.820916\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# df[\"Role\"]\n",
    "# Get unique values\n",
    "unique_roles = df['Role'].unique()\n",
    "\n",
    "# Count how many unique values there are\n",
    "num_unique_roles = df['Role'].nunique()\n",
    "\n",
    "# Get value counts (frequency of each unique value)\n",
    "role_counts = df['Role'].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of unique roles: {num_unique_roles}\")\n",
    "print(\"\\nTop 10 most common roles:\")\n",
    "print(role_counts.head(10))\n",
    "\n",
    "# To see the percentage distribution\n",
    "role_percentage = df['Role'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage distribution (top 10):\")\n",
    "print(role_percentage.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661b78c-27ce-4cf2-9763-58061c947e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Trained Successfully!\n",
      "📊 MSE: 9.8282\n",
      "📈 R² Score: 0.9379\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert Resume & Job Description to BERT embeddings\n",
    "df[\"Resume_Embeddings\"] = df[\"Resume_Clean\"].apply(lambda x: bert_model.encode(x))\n",
    "df[\"Job_Desc_Embeddings\"] = df[\"Job_Desc_Clean\"].apply(lambda x: bert_model.encode(x))\n",
    "\n",
    "# Encode \"Role\" column\n",
    "role_encoder = LabelEncoder()\n",
    "df[\"Role_Encoded\"] = role_encoder.fit_transform(df[\"Role\"])\n",
    "\n",
    "# Convert embeddings to NumPy array\n",
    "X_resumes = np.array(df[\"Resume_Embeddings\"].tolist())\n",
    "X_job_descs = np.array(df[\"Job_Desc_Embeddings\"].tolist())\n",
    "\n",
    "# Stack Resume, Job Description, and Role together\n",
    "X = np.hstack((X_resumes, X_job_descs, df[\"Role_Encoded\"].values.reshape(-1, 1)))\n",
    "y = df[\"ATS_Score\"].values  # Target variable\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost Model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=6)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Model Performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"✅ Model Trained Successfully!\")\n",
    "print(f\"📊 MSE: {mse:.4f}\")\n",
    "print(f\"📈 R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d03ea9-52a9-41b1-b73a-fc9874e9ac21",
   "metadata": {},
   "source": [
    "# Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00e1a2-249f-4d67-9ad0-319cdcb59106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['role_encoder.pkl']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(xgb_model, \"ats_model.pkl\")  \n",
    "joblib.dump(role_encoder, \"role_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc45159-06c4-452b-ad0e-7246b43d7e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e9bf7-40a4-4e7c-82a4-21254bfa9032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aade477-7c66-4773-90f1-83bfba5cf21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019741d6-5242-4d8c-8a49-9f19db65fef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceccd76-7006-4169-848b-2bdbc782bcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df550ce-c3e5-4562-942e-12e54706a705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149fdd34-863b-48e0-ad88-502c07d7e396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb0318-2595-467a-94ab-d9cb30d00e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633ee70-983f-46e9-b467-da06fd469ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdaed7-1d61-4be9-9eac-bcdd0bf49806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae4229-da23-42f9-8c9d-43b82444171e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c230176-c85c-4d3e-932e-1e25ce3872ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ebea7-ec9b-4540-a62b-f61cd0e100f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691045d8-163a-4450-9c04-256304a0fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 👉 Higher similarity scores mean a resume is a better match for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc9875-e8d9-4cb7-933e-eb31c37385a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Name                        Role  Resume_Job_Similarity\n",
      "0      Jason Jones       E-commerce Specialist               0.122216\n",
      "1     Ann Marshall              Game Developer               0.061060\n",
      "2  Patrick Mcclain  Human Resources Specialist               0.164754\n",
      "3    Patricia Gray       E-commerce Specialist               0.093921\n",
      "4     Amanda Gross       E-commerce Specialist               0.056634\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Compute similarity between Resume and Job Description\n",
    "# similarity_scores = cosine_similarity(resume_tfidf, job_desc_tfidf)\n",
    "\n",
    "# # Convert similarity matrix to DataFrame\n",
    "# df[\"Resume_Job_Similarity\"] = [similarity_scores[i, i] for i in range(len(df))]\n",
    "\n",
    "# # View similarity scores\n",
    "# print(df[[\"Name\", \"Role\", \"Resume_Job_Similarity\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e48164-9d69-4002-878e-6261c3345912",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48697788697788696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.47      0.48      1034\n",
      "           1       0.48      0.51      0.49      1001\n",
      "\n",
      "    accuracy                           0.49      2035\n",
      "   macro avg       0.49      0.49      0.49      2035\n",
      "weighted avg       0.49      0.49      0.49      2035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# # Encode target variable (convert \"select\" and \"reject\" to 1 and 0)\n",
    "# df[\"decision\"] = df[\"decision\"].map({\"select\": 1, \"reject\": 0})\n",
    "\n",
    "# # Select features and target\n",
    "# X = df[[\"Resume_Job_Similarity\"]]  # You can add more features later\n",
    "# y = df[\"decision\"]\n",
    "\n",
    "# # Split into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train a model (Random Forest)\n",
    "# model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate model performance\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d8fb4-9fc2-4f3d-9e49-8f18c1aa1d39",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Name                        Role  decision  ATS_Score\n",
      "0      Jason Jones       E-commerce Specialist         0   9.226798\n",
      "1     Ann Marshall              Game Developer         1   5.624799\n",
      "2  Patrick Mcclain  Human Resources Specialist         0  16.415510\n",
      "3    Patricia Gray       E-commerce Specialist         1   7.105350\n",
      "4     Amanda Gross       E-commerce Specialist         0   3.949370\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Initialize TF-IDF Vectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Fit & transform Resume and Job Description\n",
    "# tfidf_resume = tfidf_vectorizer.fit_transform(df[\"Resume_Clean\"])\n",
    "# tfidf_job_desc = tfidf_vectorizer.transform(df[\"Job_Desc_Clean\"])\n",
    "\n",
    "# # Compute Cosine Similarity\n",
    "# similarity_scores = cosine_similarity(tfidf_resume, tfidf_job_desc)\n",
    "\n",
    "# # Normalize similarity score to a percentage (0-100)\n",
    "# df[\"ATS_Score\"] = similarity_scores.diagonal() * 100\n",
    "\n",
    "# # Display results\n",
    "# print(df[[\"Name\", \"Role\", \"decision\", \"ATS_Score\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b00b7-0235-4e4b-b1f1-eb3c17c24d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Combine resume and job description text for vectorization\n",
    "# all_text = df[\"Resume_Clean\"].tolist() + df[\"Job_Desc_Clean\"].tolist()\n",
    "\n",
    "# # Initialize TF-IDF Vectorizer on combined text\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit features for better accuracy\n",
    "# tfidf_vectorizer.fit(all_text)\n",
    "\n",
    "# # Transform resumes and job descriptions\n",
    "# tfidf_resume = tfidf_vectorizer.transform(df[\"Resume_Clean\"])\n",
    "# tfidf_job_desc = tfidf_vectorizer.transform(df[\"Job_Desc_Clean\"])\n",
    "\n",
    "# # Compute Cosine Similarity\n",
    "# similarity_scores = cosine_similarity(tfidf_resume, tfidf_job_desc)\n",
    "\n",
    "# # Normalize similarity score to percentage (0-100)\n",
    "# df[\"ATS_Score\"] = similarity_scores.diagonal() * 100\n",
    "\n",
    "# # Display results\n",
    "# print(df[[\"Name\", \"Role\", \"decision\", \"ATS_Score\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fa14f-a3ec-4d7c-a1c2-a490da1df1ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Role</th>\n",
       "      <th>Transcript</th>\n",
       "      <th>Resume</th>\n",
       "      <th>decision</th>\n",
       "      <th>Reason_for_decision</th>\n",
       "      <th>Job_Description</th>\n",
       "      <th>Resume_Clean</th>\n",
       "      <th>Job_Desc_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jasojo159</td>\n",
       "      <td>Jason Jones</td>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Interviewer: Good morning, Jason. It's great t...</td>\n",
       "      <td>Here's a professional resume for Jason Jones:\\...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Lacked leadership skills for a senior position.</td>\n",
       "      <td>Be part of a passionate team at the forefront ...</td>\n",
       "      <td>professional resume jason jones jason jones e ...</td>\n",
       "      <td>part passionate team forefront machine learn e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>annma759</td>\n",
       "      <td>Ann Marshall</td>\n",
       "      <td>Game Developer</td>\n",
       "      <td>Interview Scene\\n\\nA conference room with a ta...</td>\n",
       "      <td>Here's a professional resume for Ann Marshall:...</td>\n",
       "      <td>select</td>\n",
       "      <td>Strong technical skills in AI and ML.</td>\n",
       "      <td>Help us build the next-generation products as ...</td>\n",
       "      <td>professional resume ann marshall ann marshall ...</td>\n",
       "      <td>help we build next generation product game dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patrmc729</td>\n",
       "      <td>Patrick Mcclain</td>\n",
       "      <td>Human Resources Specialist</td>\n",
       "      <td>Interview Setting: A conference room in a medi...</td>\n",
       "      <td>Here's a professional resume for Patrick Mccla...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Insufficient system design expertise for senio...</td>\n",
       "      <td>We need a Human Resources Specialist to enhanc...</td>\n",
       "      <td>professional resume patrick mcclain patrick mc...</td>\n",
       "      <td>need human resource specialist enhance team te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patrgr422</td>\n",
       "      <td>Patricia Gray</td>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Here's a simulated professional interview for ...</td>\n",
       "      <td>Here's a professional resume for Patricia Gray...</td>\n",
       "      <td>select</td>\n",
       "      <td>Impressive leadership and communication abilit...</td>\n",
       "      <td>Be part of a passionate team at the forefront ...</td>\n",
       "      <td>professional resume patricia gray patricia gra...</td>\n",
       "      <td>part passionate team forefront cloud computing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amangr696</td>\n",
       "      <td>Amanda Gross</td>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Here's the simulated interview:\\n\\nInterviewer...</td>\n",
       "      <td>Here's a professional resume for Amanda Gross:...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Lacked leadership skills for a senior position.</td>\n",
       "      <td>We are looking for an experienced E-commerce S...</td>\n",
       "      <td>professional resume amanda gross amanda gross ...</td>\n",
       "      <td>look experience e commerce specialist join tea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID             Name                        Role  \\\n",
       "0  jasojo159      Jason Jones       E-commerce Specialist   \n",
       "1   annma759     Ann Marshall              Game Developer   \n",
       "2  patrmc729  Patrick Mcclain  Human Resources Specialist   \n",
       "3  patrgr422    Patricia Gray       E-commerce Specialist   \n",
       "4  amangr696     Amanda Gross       E-commerce Specialist   \n",
       "\n",
       "                                          Transcript  \\\n",
       "0  Interviewer: Good morning, Jason. It's great t...   \n",
       "1  Interview Scene\\n\\nA conference room with a ta...   \n",
       "2  Interview Setting: A conference room in a medi...   \n",
       "3  Here's a simulated professional interview for ...   \n",
       "4  Here's the simulated interview:\\n\\nInterviewer...   \n",
       "\n",
       "                                              Resume decision  \\\n",
       "0  Here's a professional resume for Jason Jones:\\...   reject   \n",
       "1  Here's a professional resume for Ann Marshall:...   select   \n",
       "2  Here's a professional resume for Patrick Mccla...   reject   \n",
       "3  Here's a professional resume for Patricia Gray...   select   \n",
       "4  Here's a professional resume for Amanda Gross:...   reject   \n",
       "\n",
       "                                 Reason_for_decision  \\\n",
       "0    Lacked leadership skills for a senior position.   \n",
       "1              Strong technical skills in AI and ML.   \n",
       "2  Insufficient system design expertise for senio...   \n",
       "3  Impressive leadership and communication abilit...   \n",
       "4    Lacked leadership skills for a senior position.   \n",
       "\n",
       "                                     Job_Description  \\\n",
       "0  Be part of a passionate team at the forefront ...   \n",
       "1  Help us build the next-generation products as ...   \n",
       "2  We need a Human Resources Specialist to enhanc...   \n",
       "3  Be part of a passionate team at the forefront ...   \n",
       "4  We are looking for an experienced E-commerce S...   \n",
       "\n",
       "                                        Resume_Clean  \\\n",
       "0  professional resume jason jones jason jones e ...   \n",
       "1  professional resume ann marshall ann marshall ...   \n",
       "2  professional resume patrick mcclain patrick mc...   \n",
       "3  professional resume patricia gray patricia gra...   \n",
       "4  professional resume amanda gross amanda gross ...   \n",
       "\n",
       "                                      Job_Desc_Clean  \n",
       "0  part passionate team forefront machine learn e...  \n",
       "1  help we build next generation product game dev...  \n",
       "2  need human resource specialist enhance team te...  \n",
       "3  part passionate team forefront cloud computing...  \n",
       "4  look experience e commerce specialist join tea...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ec4b0-eb4b-406f-9bcc-64a61bfef556",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Name                        Role decision  ATS_Score\n",
      "0      Jason Jones       E-commerce Specialist   reject  23.335724\n",
      "1     Ann Marshall              Game Developer   select  16.846628\n",
      "2  Patrick Mcclain  Human Resources Specialist   reject  31.382445\n",
      "3    Patricia Gray       E-commerce Specialist   select  20.211019\n",
      "4     Amanda Gross       E-commerce Specialist   reject  23.006243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ## 🔹 Step 1: Train TF-IDF on Resume + Job Description ##\n",
    "# all_text = df[\"Resume_Clean\"].tolist() + df[\"Job_Desc_Clean\"].tolist()\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# tfidf_vectorizer.fit(all_text)\n",
    "\n",
    "# # Transform resumes and job descriptions\n",
    "# tfidf_resume = tfidf_vectorizer.transform(df[\"Resume_Clean\"])\n",
    "# tfidf_job_desc = tfidf_vectorizer.transform(df[\"Job_Desc_Clean\"])\n",
    "\n",
    "# # Compute TF-IDF Cosine Similarity\n",
    "# tfidf_similarity_scores = cosine_similarity(tfidf_resume, tfidf_job_desc).diagonal()\n",
    "\n",
    "# ## 🔹 Step 2: Compute Weighted Keyword Matching ##\n",
    "# idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "# important_words = sorted(idf_scores, key=idf_scores.get, reverse=True)[:50]  # Top 50 keywords\n",
    "\n",
    "# def weighted_keyword_match(resume, job_desc):\n",
    "#     resume_words = set(resume.split())\n",
    "#     job_words = set(job_desc.split())\n",
    "#     common_words = resume_words.intersection(job_words)\n",
    "#     return sum(idf_scores.get(word, 0) for word in common_words) / len(important_words)\n",
    "\n",
    "# df[\"Weighted_Match_Score\"] = df.apply(lambda row: weighted_keyword_match(row[\"Resume_Clean\"], row[\"Job_Desc_Clean\"]), axis=1)\n",
    "\n",
    "# ## 🔹 Step 3: Use BERT for Semantic Similarity ##\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# resume_embeddings = model.encode(df[\"Resume_Clean\"].tolist(), convert_to_tensor=True)\n",
    "# job_desc_embeddings = model.encode(df[\"Job_Desc_Clean\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# bert_similarity_scores = cosine_similarity(resume_embeddings.cpu(), job_desc_embeddings.cpu()).diagonal()\n",
    "\n",
    "# ## 🔹 Step 4: Combine All Scores ##\n",
    "# df[\"ATS_Score\"] = (0.4 * tfidf_similarity_scores) + (0.3 * df[\"Weighted_Match_Score\"]) + (0.3 * bert_similarity_scores)\n",
    "# df[\"ATS_Score\"] = df[\"ATS_Score\"] * 100  # Normalize to percentage scale\n",
    "\n",
    "# # Display Results\n",
    "# print(df[[\"Name\", \"Role\", \"decision\", \"ATS_Score\"]].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b668479-a185-493b-ad35-ef6608a8a28b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Resume_Clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Resume_Clean'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 🔹 **TF-IDF Feature Extraction with Improved Parameters**\u001b[39;00m\n\u001b[32m      4\u001b[39m tfidf_vectorizer = TfidfVectorizer(max_features=\u001b[32m5000\u001b[39m, min_df=\u001b[32m5\u001b[39m, ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m tfidf_vectorizer.fit(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResume_Clean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.tolist() + df[\u001b[33m\"\u001b[39m\u001b[33mJob_Desc_Clean\u001b[39m\u001b[33m\"\u001b[39m].tolist())\n\u001b[32m      7\u001b[39m tfidf_resume = tfidf_vectorizer.transform(df[\u001b[33m\"\u001b[39m\u001b[33mResume_Clean\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      8\u001b[39m tfidf_job_desc = tfidf_vectorizer.transform(df[\u001b[33m\"\u001b[39m\u001b[33mJob_Desc_Clean\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Resume_Clean'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 🔹 **TF-IDF Feature Extraction with Improved Parameters**\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=5, ngram_range=(1,2))\n",
    "tfidf_vectorizer.fit(df[\"Resume_Clean\"].tolist() + df[\"Job_Desc_Clean\"].tolist())\n",
    "\n",
    "tfidf_resume = tfidf_vectorizer.transform(df[\"Resume_Clean\"])\n",
    "tfidf_job_desc = tfidf_vectorizer.transform(df[\"Job_Desc_Clean\"])\n",
    "\n",
    "tfidf_similarity_scores = cosine_similarity(tfidf_resume, tfidf_job_desc).diagonal()\n",
    "\n",
    "# 🔹 **Weighted Keyword Matching (Domain-Specific Enhancement)**\n",
    "idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "\n",
    "def weighted_keyword_match(resume, job_desc):\n",
    "    resume_words = set(resume.split())\n",
    "    job_words = set(job_desc.split())\n",
    "    common_words = resume_words.intersection(job_words)\n",
    "    return sum(idf_scores.get(word, 0) for word in common_words) / max(1, len(common_words))\n",
    "\n",
    "df[\"Weighted_Match_Score\"] = df.apply(lambda row: weighted_keyword_match(row[\"Resume_Clean\"], row[\"Job_Desc_Clean\"]), axis=1)\n",
    "\n",
    "# 🔹 **BERT Embeddings with Normalization**\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "resume_embeddings = model.encode(df[\"Resume_Clean\"].tolist(), normalize_embeddings=True)\n",
    "job_desc_embeddings = model.encode(df[\"Job_Desc_Clean\"].tolist(), normalize_embeddings=True)\n",
    "\n",
    "bert_similarity_scores = cosine_similarity(resume_embeddings, job_desc_embeddings).diagonal()\n",
    "\n",
    "# 🔹 **Normalize All Scores Before Combining**\n",
    "scaler = MinMaxScaler()\n",
    "df[\"TFIDF_Score\"] = scaler.fit_transform(tfidf_similarity_scores.reshape(-1, 1)).flatten()\n",
    "df[\"Weighted_Score\"] = scaler.fit_transform(df[\"Weighted_Match_Score\"].values.reshape(-1, 1)).flatten()\n",
    "df[\"BERT_Score\"] = scaler.fit_transform(bert_similarity_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 🔹 **Final ATS Score Calculation (Adjusted Weights)**\n",
    "df[\"ATS_Score\"] = (0.5 * df[\"BERT_Score\"]) + (0.3 * df[\"TFIDF_Score\"]) + (0.2 * df[\"Weighted_Score\"])\n",
    "df[\"ATS_Score\"] = df[\"ATS_Score\"] * 100  # Convert to percentage scale\n",
    "\n",
    "# 🔹 **Display Results**\n",
    "print(df[[\"Name\", \"Role\", \"decision\", \"ATS_Score\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854a275-83ae-47f0-97c5-84ae95c5f9cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# ✅ **TF-IDF With Feature Selection**\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=7000, min_df=3, ngram_range=(1,3))\n",
    "tfidf_vectorizer.fit(df[\"Resume_Clean\"].tolist() + df[\"Job_Desc_Clean\"].tolist())\n",
    "\n",
    "tfidf_resume = tfidf_vectorizer.transform(df[\"Resume_Clean\"])\n",
    "tfidf_job_desc = tfidf_vectorizer.transform(df[\"Job_Desc_Clean\"])\n",
    "tfidf_similarity_scores = (tfidf_resume @ tfidf_job_desc.T).diagonal()\n",
    "\n",
    "# ✅ **BERT-Based Sentence Similarity (Instead of Cosine Similarity)**\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "resume_embeddings = model.encode(df[\"Resume_Clean\"].tolist(), normalize_embeddings=True)\n",
    "job_desc_embeddings = model.encode(df[\"Job_Desc_Clean\"].tolist(), normalize_embeddings=True)\n",
    "\n",
    "bert_similarity_scores = (resume_embeddings * job_desc_embeddings).sum(axis=1)\n",
    "\n",
    "# ✅ **Cross-Encoder for Improved Semantic Matching**\n",
    "cross_encoder = CrossEncoder('cross-encoder/stsb-TinyBERT-L-4')\n",
    "cross_inputs = [[r, j] for r, j in zip(df[\"Resume_Clean\"], df[\"Job_Desc_Clean\"])]\n",
    "cross_scores = cross_encoder.predict(cross_inputs)\n",
    "\n",
    "# ✅ **Scaling All Scores**\n",
    "scaler = MinMaxScaler()\n",
    "df[\"TFIDF_Score\"] = scaler.fit_transform(tfidf_similarity_scores.reshape(-1, 1)).flatten()\n",
    "df[\"BERT_Score\"] = scaler.fit_transform(bert_similarity_scores.reshape(-1, 1)).flatten()\n",
    "df[\"Cross_Score\"] = scaler.fit_transform(np.array(cross_scores).reshape(-1, 1)).flatten()\n",
    "\n",
    "# ✅ **Final ATS Score Calculation (Cross-Encoder Weighted Higher)**\n",
    "df[\"ATS_Score\"] = (0.6 * df[\"Cross_Score\"]) + (0.3 * df[\"BERT_Score\"]) + (0.1 * df[\"TFIDF_Score\"])\n",
    "df[\"ATS_Score\"] = df[\"ATS_Score\"] * 100  # Convert to percentage\n",
    "\n",
    "# ✅ **Display Results**\n",
    "print(df[[\"Name\", \"Role\", \"decision\", \"ATS_Score\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58c2999c-7b5e-455a-8c22-70d3468c192f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load the trained ATS model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m ats_model = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mats_model.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load the role encoder\u001b[39;00m\n\u001b[32m      7\u001b[39m role_encoder = joblib.load(\u001b[33m\"\u001b[39m\u001b[33mrole_encoder.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/joblib/numpy_pickle.py:749\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    744\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[32m    746\u001b[39m             \u001b[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001b[39;00m\n\u001b[32m    747\u001b[39m             \u001b[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001b[39;00m\n\u001b[32m    748\u001b[39m             \u001b[38;5;66;03m# native endianness of the host system.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m             obj = \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m                \u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidated_mmap_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyProjects/ats-score-model/.venv/lib/python3.13/site-packages/joblib/numpy_pickle.py:626\u001b[39m, in \u001b[36m_unpickle\u001b[39m\u001b[34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[39m\n\u001b[32m    624\u001b[39m obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     obj = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m unpickler.compat_mode:\n\u001b[32m    628\u001b[39m         warnings.warn(\n\u001b[32m    629\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe file \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has been generated with a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    630\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mjoblib version less than 0.10. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    633\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    634\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pickle.py:1256\u001b[39m, in \u001b[36m_Unpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1254\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pickle.py:1581\u001b[39m, in \u001b[36m_Unpickler.load_stack_global\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1579\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1580\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[33m\"\u001b[39m\u001b[33mSTACK_GLOBAL requires str\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1581\u001b[39m \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pickle.py:1622\u001b[39m, in \u001b[36m_Unpickler.find_class\u001b[39m\u001b[34m(self, module, name)\u001b[39m\n\u001b[32m   1620\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m _compat_pickle.IMPORT_MAPPING:\n\u001b[32m   1621\u001b[39m         module = _compat_pickle.IMPORT_MAPPING[module]\n\u001b[32m-> \u001b[39m\u001b[32m1622\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proto >= \u001b[32m4\u001b[39m:\n\u001b[32m   1624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys.modules[module], name)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the trained ATS model\n",
    "ats_model = joblib.load(\"ats_model.pkl\")\n",
    "\n",
    "# Load the role encoder\n",
    "role_encoder = joblib.load(\"role_encoder.pkl\")\n",
    "\n",
    "\n",
    "role_encoded = role_encoder.transform([\"Software Engineer\"])  # Example role encoding\n",
    "predicted_ats_score = ats_model.predict([[role_encoded, resume_vectorized, job_desc_vectorized]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43798371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
